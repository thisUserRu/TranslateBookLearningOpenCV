## (П]|(РС)|(РП) Исключение фона

Из-за своей простоты и т.к. камера, как правило, зафиксирована, *исключение фона*, вероятно, наиболее фундаментальная операция обработки изображения из видеопотока для обеспечения безопасности. Для выполнения исключения фона для начала необходимо "изучить" модель фона. Однажды изученная *модель фона* сравнивается с текущим изображением с последующим исключением известных частей фона. Объекты, оставшиеся после исключения, предположительно будут являться новыми объектами переднего плана.

Конечно, "фон" является плохо определенным понятием, которое изменяется в зависимости от применения. Например, в случае рассмотрения шоссе, возможно, обычный транспортный поток следует считать фоном. Как правило, за фон принимаются любые статические или периодически движущие части сцены, которые остаются неизменными или периодическими в течение интересующего периода. Группа может иметь изменяющиеся во времени компоненты, такие как деревья, которые развиваются от ветра утром и вечером, но неподвижны в полдень. Две распространённые, но существенно различающиеся категории окружения, которые могут встретиться, это сцены внутри и снаружи помещений. Возникает интерес в инструментах, которые помогут в обеих категориях окружения. Вначале будут рассмотрены недостатки типичных моделей фона, а затем будут рассмотрены модели сцен высших порядков. Далее будет представлен быстрый метод, который в основном хорош для статичных фоновых сцен внутри помещений, освещение которых практически не меняется. Затем будет рассмотрен метод "кодовых книг", который немного медленнее, однако может работать в сценах и внутри и снаружи помещений; этот метод подходит для периодических движений (таких, как раскачивающихся на ветру деревьев) и для медленного или периодически изменяемого освещения. Этот метод так же устойчив при изучении фона, даже когда есть случайные объекты переднего плана, движущиеся мимо. Ранее эта тема уже была затронута во время обсуждения связанных компонентов (впервые в главе 5) в контексте обнаружения объекта переднего плана. И в завершении, будет представлено сравнение быстрого метода исключения фона с методом "кодовой книги".

### Слабые стороны исключения фона

Хотя методы моделирования фона, упомянутые здесь, работают достаточно хорошо для простых сцен, однако, они страдают от предположения, которое часто нарушается: что все пиксели независимы. Рассматриваемые методы обучают модель изменения пикселей без учета соседних пикселей. Для принятия окружающих пикселей во внимание, необходимо изучить модель, состоящую из нескольких частей; простым примером такой модели будет расширенная основная модель независимых пикселей путем включения элементарной чувствительной к яркости соседних пикселей. В этом случае используется яркость соседних пикселей, чтобы различать случаи, когда значение соседнего пикселя будет относительно ярким или тусклым. В связи с этим существует две модели для конкретного пикселя: одна для случая, когда соседние пиксели ярче, а другая для случая, когда соседние пиксели более тусклые. В общем, имеется модель, которая принимает во внимание окружающий *контекст*. Однако это приводит к возрастанию используемой памяти в 2 раза и количеству операций вычислений, так как возникает потребность в значениях для случаев, когда окружающие пиксели либо ярче, либо более тусклые. Помимо этого необходимо в два раза больше данных, чтобы заполнить эти две модели состояний. Можно обобщить идею "высокого" и "низкого" контекстов в многомерную гистограмму интенсивности конкретного и соседнего пикселей, а также сделать её ещё более сложной, делая это за несколько временных шагов. При этом стоит принимать во внимание, что более сложная в пространстве и времени модель требует ещё больше памяти, собираемых данных и вычислительных ресурсов.

Из-за дополнительных расходов использование более сложных моделей, как правило, стараются избегать. Для более эффективного распоряжения ресурсами, можно избавляться от *ложноположительных* пикселях, которые оказывают влияние в тех случаях, когда нарушается предположение о независимости пикселей. Избавление принимает форму операций обработки изображений (как правило, *cvErode()*, *cvDilate()* и *cvFloodFill()*), которые исключают ложноположительные пиксели. Ранее эти операции уже были рассмотрены (глава 5) в контексте поиска больших и компактных (это математический термин, который не имеет ничего общего с размером) *связанных компонентов* в данных с наличием шума. В данной главе связанные компоненты так же будут использованы, а на данный момент ограничимся подходом, который предполагает независимое изменение пикселей.

### Моделирование сцены

Итак, каким же образом отделить фон от переднего плана? Например, если ведется наблюдение за стоянкой и на парковку въезжает автомобиль, то он будет являться новым объектом переднего плана. Но должен ли он оставаться объектом переднего плана навсегда? А как насчет перемещенного мусорного бака? Он будет отображаться на переднем плане в двух местах: туда, куда переместили и "дырой" в месте, откуда он был перемещен. Как объяснить эту разницу? И ещё, как долго мусорный бак (или "дыра") остается объектом переднего плана? Если смоделировать темную комнату, и кто-то вдруг включит свет, должна ли вся комната стать объектом переднего плана? Чтобы ответить на все эти вопросы, необходима высокоуровневая модель "сцены", которая определяет несколько уровней между состояниями переднего плана и фон, а также временной метод медленной передачи неподвижных объектов переднего плана фону. К тому же требуется определять и создавать новую модель при глобальных изменениях в сцене.

В общем, модель сцены может содержать несколько слоев, от "нового переднего плана" до старого переднего плана и так вплоть до фона. Также должно быть реализовано детектирование движения таким образом, чтобы при перемещении объекта  можно было идентифицировать "позитивную" часть (новое местоположение) и "негативную" часть (старое местоположение, "дыра").

Таким образом, новый объект переднего плана должен быть перемещен на уровень "новые объекты переднего плана" и отмечен как позитивный объект или как дыра. В районах, где нет объектов переднего плана, можно продолжать обновление модели фона. Если объект переднего плана не перемещался в течение заданного участка времени, то он будет перемещен на уровень "старые объекты переднего плана", где пиксельная статистика предварительно изучается до тех пор, пока изучаемая модель не присоединиться к модели фона.

Для отслеживания глобальных изменений, таких, как включение освещения в помещении, необходимо использовать глобальную разность кадров. Например, если одновременно изменениям подверглось большое количество пикселей, тогда можно классифицировать это скорее как глобальное изменение, а не локальное, а затем переключиться на использование модели для новой ситуации.

### Срез пикселей

Прежде, чем перейти к моделированию пиксельных изменений, необходимо получить представление о том, как изменяются пиксели изображения во времени. Рассмотрим случай, когда камера следит за деревом на улице, которое раскачивается на ветру. На рисунке 9-1 показано, как выглядят пиксели выбранного линейного сегмента на протяжении 60 кадров. Зададимся целью получения модели этих колебаний. Однако, прежде, сделаем небольшое отступление, чтобы обсудить способ получения этой линии, потому что в целом это полезный прием для генерации признаков и для отладки. 

![Рисунок 9-1 не найден](Images/Pic_9_1.jpg)

Рисунок 9-1. Колебание пикселей линии на протяжении 60 кадров из сцены раскачивающегося дерева: некоторые темные области (вверху слева) довольно таки стабильны, тогда как в области движущихся ветвей (вверху в центре) могут изменяться в широких приделах

В OpenCV имеются функции, которые позволяет легко получить произвольную линию пикселей. Это функции *cvInitLineIterator()* и *CV_NEXT_LINE_POINT()*. Прототип функции *cvInitLineIterator()*:

```cpp
	int cvInitLineIterator(
		 const CvArr* 		image
		,CvPoint 			pt1
		,CvPoint 			pt2
		,CvLineIterator* 	line_iterator
		,int 				connectivity 	= 8
		,int 				left_to_right 	= 0
	);
```

Исходное изображение *images* может иметь любой тип или количество каналов. Точки *p1* и *p2* являются концами линейного сегмента. Итератор *line_iterator* отвечает за перемещение между точками вдоль линии. В случае использования многоканальных изображений, каждый вызов *CV_NEXT_LINE_POINT()* перемещает *line_iterator* к следующему пикселю. Для получения доступа ко всем каналам необходимо использовать *line_iterator.ptr[0]*, *line_iterator.ptr[1]* и так далее. Связность *connectivity* может быть равна 4 (линия может совершать шаги вверх, вниз, влево, вправо) или 8 (линия может дополнительно делать шаги по диагоналям). Если *left_to_right* равен 0 (false), тогда *line_iterator* совершает шаги от *p1* до *p2*; иначе, шаги будут осуществляться от крайней левой точки к крайней правой. (Флаг *left_to_right* был введен из-за того, что дискретная линия, проведенная от *pt1* к *pt2*, не всегда соответствует линии, проведенной от *pt2* к *pt1*. Таким образом, установка этого флага дает пользователю получить точную растеризацию в независимости от последовательности *pt1*, *pt2*). Функция *cvInitLineIterator()* возвращает число точек, которые были пройдены для этой линии. Сопутствующий макрос *CV_NEXT_LINE_POINT(line_iterator)* перемещает итератор от одного пикселя к другому.

Прервемся от обсуждения и посмотрим на то, как этот метод может быть использован для извлечения некоторых данных из файла (пример 9-1). При этом переосмыслим рисунок 9-1 с точки зрения полученных данных из файла.

Пример 9-1. Чтение RGB значений всех пикселей одной строки файла и сохранение этих значений в трех отдельных файлах

```cpp
// Сохранение на диск линейного сегмента из BGR пикселей от p1 до p2
//
CvCapture* 		capture = cvCreateFileCapture( argv[1] );
int 			max_buffer;
IplImage* 		rawImage;
int 			r[10000], g[10000], b[10000];
CvLineIterator 	iterator;

FILE *fptrb = fopen("blines.csv","w"); 	// Создание файлов для сохранения 
FILE *fptrg = fopen("glines.csv","w"); 	// каждого из каналов в отдельности
FILE *fptrr = fopen("rlines.csv","w");

// Главный цикл обработки
//
for(;;) {

	if( !cvGrabFrame( capture ))
		break;

	rawImage = cvRetrieveFrame( capture );
	max_buffer = cvInitLineIterator(rawImage,pt1,pt2,&iterator,8,0);
	for(int j=0; j<max_buffer; j++){

		// Запись значений
		// 

		fprintf(fptrb, "%d,", iterator.ptr[0]); // синий
		fprintf(fptrg, "%d,", iterator.ptr[1]); // зеленый
		fprintf(fptrr, "%d,", iterator.ptr[2]); // красный
		
		iterator.ptr[2] = 255; // Маркировка этого образца красным

		CV_NEXT_LINE_POINT(iterator); // Переход к следующему пикселю
	}

	// Вывод данных в строках
	//
	fprintf(fptrb, "/n"); fprintf(fptrg,"/n"); fprintf(fptrr, "/n");
}

// Очистка
//
fclose(fptrb); fclose(fptrg); fclose(fptrr);
cvReleaseCapture( &capture );
```

Получить линию выборки можно ещё проще, а именно:

```cpp
	int cvSampleLine(
		 const CvArr* 	image
		,CvPoint 		pt1
		,CvPoint 		pt2
		,void* 			buffer
		,int 			connectivity = 8
	);
```

Эта функция просто обертка функции *cvInitLineIterator()* вместе с макросом *CV_NEXT_LINE_POINT(line_iterator)*. Она производит выборку от *pt1* до *pt2*; затем передается указатель *buffer* нужного типа и длиной ![Формула 9-1 не найдена](Images/Frml_9_1.jpg). Так же как и линейный итератор, *cvSampleLine()* пошагово проходит по кадому каналу каждого пикселя многоканального изображения, прежде, чем переместиться к следующему пикселю. Функция возвращает число элементов *buffer*.

Теперь можно переходить к рассмотрению методов моделирования колебаний пикселей увиденных на рисунке 9-1. По мере продвижения от простой к более сложной модели, будут представлены лишь те, которые работают в режиме реального времени и в рамках разумных ограничений по памяти.

### Выявление отличительных признаков кадров

Самым простым способом исключения фона является вычитание одного кадра из другого (возможно расположенного несколько позже) с последующим выделением любой разницы, которая "достаточно большая" на переднем плане. Этот процесс стремиться поймать границы движущихся объектов. Для простоты рассмотрим три одноканальных изображения: *frameTime1*, *frameTime2* и *frameForeground*. Во *frameTime1* поместим предыдущий кадр в оттенках серого, а в *frameTime2* текущий кадр в оттенках серого. Затем, воспользовавшись *cvAdsDiff()*, вычислим (в абсолютных значениях) разницу между передними планами и поместим результат в *frameForeground*.

```cpp
	cvAbsDiff(
		 frameTime1
		,frameTime2
		,frameForeground
	);
```

Так как в значениях пикселя всегда присутствует шум и колебания, необходимо игнорировать (устанавливать в 0) малые различия (например, меньше 15) и помечать остальные как большие различия (устанавливать в 255).

```cpp
	cvThreshold(
		 frameForeground
		,frameForeground
		,15
		,255
		,CV_THRESH_BINARY
	);
```

Таким образом изображение *frameForeground* будет содержать кондидатов на объекты переднего плана, отмеченные пикселями со значениями равными 255 и фон, отмеченный пикселями со значениями 0. Теперь, как уже было сказано ранее, необходимо избавиться от мелких шумовых областей; для этого можно использовать функцию *cvErode()* или связанную компоненту. Для цветных изображений данный подход также может использовать, но применять его к каждому каналу в отдельности с последующим соединением их обратно с помощью *cvOr()*. Этот метод является слишком простым для большинства приложений и позволяет отмечать лишь области движения. Для получения более эффективной модели фона, необходимо накапливать некоторую статистику о средних значениях и средних различиях между пикселями сцены. Забегая немного вперед, можно посмотреть примеры различий между кадрами на рисунке 9-5 и рисунке 9-6 в разделе "Быстрый тест".

### Метод усреднения фона

